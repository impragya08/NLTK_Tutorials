{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition\n",
    "NER is the process of identifying and extracting named entities from text, such as person names, organization names, locations, dates, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import nltk\n",
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text= state_union.raw(\"2005-GWBush.txt\")\n",
    "sample_text= state_union.raw(\"2006-GWBush.txt\")\n",
    "\n",
    "custom_sent_tokenizer= PunktSentenceTokenizer(sample_text)\n",
    "\n",
    "tokenized= custom_sent_tokenizer.tokenize(sample_text)\n",
    "\n",
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized:\n",
    "            # tags each word with its part of speech\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            #it just remove period like it now considers White house now same\n",
    "            namedEnt=nltk.ne_chunk(tagged, binary= True)\n",
    "            namedEnt.draw()\n",
    "        \n",
    "\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "process_content()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatizing\n",
    "Lemmatizing is a natural language processing technique that involves reducing words to their base or dictionary form, which is called a \"lemma\".\n",
    "Lemmatizing is similar to stemming, but it involves using a dictionary or knowledge of the language to properly transform words to their base form. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why Lemmatizing?\n",
    "Lemmatizing is useful in natural language processing tasks such as text classification, sentiment analysis, and information retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat\n",
      "cactus\n",
      "goose\n",
      "rock\n",
      "python\n",
      "good\n",
      "run\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer= WordNetLemmatizer()\n",
    "print(lemmatizer.lemmatize(\"cats\"))\n",
    "print(lemmatizer.lemmatize(\"cacti\"))\n",
    "print(lemmatizer.lemmatize(\"geese\"))\n",
    "print(lemmatizer.lemmatize(\"rocks\"))\n",
    "print(lemmatizer.lemmatize(\"python\"))\n",
    "#default parameter for pos is noun{n}\n",
    "print(lemmatizer.lemmatize(\"better\",pos=\"a\"))\n",
    "print(lemmatizer.lemmatize(\"run\",pos=\"v\"))\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpora\n",
    "In natural language processing, a corpus (plural: corpora) refers to a large and structured set of text or speech data that is used for language analysis and research. A corpus can include any type of written or spoken language, such as books, newspapers, blogs, social media posts, speeches, and more."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why Copora?\n",
    "Corpora are important in natural language processing because they provide a large and diverse set of data that can be used to train machine learning algorithms, test hypotheses about language use and structure, and build language models. Corpora can also be used to study patterns of language use across different contexts, such as historical periods or geographic regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1:5 And God called the light Day, and the darkness he called Night.', 'And the evening and the morning were the first day.', '1:6 And God said, Let there be a firmament in the midst of the waters,\\nand let it divide the waters from the waters.', '1:7 And God made the firmament, and divided the waters which were\\nunder the firmament from the waters which were above the firmament:\\nand it was so.', '1:8 And God called the firmament Heaven.', 'And the evening and the\\nmorning were the second day.', '1:9 And God said, Let the waters under the heaven be gathered together\\nunto one place, and let the dry land appear: and it was so.', '1:10 And God called the dry land Earth; and the gathering together of\\nthe waters called he Seas: and God saw that it was good.', '1:11 And God said, Let the earth bring forth grass, the herb yielding\\nseed, and the fruit tree yielding fruit after his kind, whose seed is\\nin itself, upon the earth: and it was so.', '1:12 And the earth brought forth grass, and herb yielding seed after\\nhis kind, and the tree yielding fruit, whose seed was in itself, after\\nhis kind: and God saw that it was good.']\n"
     ]
    }
   ],
   "source": [
    "#Finding the corpus\n",
    "from nltk.corpus import gutenberg\n",
    "sample= gutenberg.raw(\"bible-kjv.txt\")\n",
    "tok=sent_tokenize(sample)\n",
    "print(tok[5:15])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WordNet\n",
    "It is a large database of English words and their semantic relationships, organized into sets of synonyms, or \"synsets\".\n",
    "It provides a powerful tool for identifying relationships between words and concepts, and can help improve the accuracy and effectiveness of natural language processing algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('plan.n.01'), Synset('program.n.02'), Synset('broadcast.n.02'), Synset('platform.n.02'), Synset('program.n.05'), Synset('course_of_study.n.01'), Synset('program.n.07'), Synset('program.n.08'), Synset('program.v.01'), Synset('program.v.02')]\n",
      "Synset('plan.n.01')\n",
      "[Lemma('plan.n.01.plan'), Lemma('plan.n.01.program'), Lemma('plan.n.01.programme')]\n",
      "plan\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "syns=wordnet.synsets(\"program\")\n",
    "print(syns)\n",
    "print(syns[0])\n",
    "print(syns[0].lemmas())\n",
    "print(syns[0].lemmas()[0].name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a series of steps to be carried out or goals to be accomplished\n",
      "['they drew up a six-step plan', 'they discussed plans for a new bond issue']\n"
     ]
    }
   ],
   "source": [
    "#definition\n",
    "print(syns[0].definition())\n",
    "\n",
    "#examples\n",
    "print(syns[0].examples())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dependable', 'upright', 'good', 'goodness', 'unspoilt', 'ripe', 'unspoiled', 'adept', 'salutary', 'skilful', 'expert', 'trade_good', 'well', 'honest', 'respectable', 'just', 'proficient', 'soundly', 'estimable', 'secure', 'sound', 'thoroughly', 'effective', 'near', 'full', 'right', 'beneficial', 'honorable', 'serious', 'dear', 'commodity', 'practiced', 'safe', 'skillful', 'in_effect', 'undecomposed', 'in_force'}\n",
      "{'bad', 'ill', 'evilness', 'badness', 'evil'}\n"
     ]
    }
   ],
   "source": [
    "synonyms=[]\n",
    "antonyms=[]\n",
    "\n",
    "for syn in wordnet.synsets(\"good\"):\n",
    "    for l in syn.lemmas():\n",
    "        synonyms.append(l.name())\n",
    "        if l.antonyms():\n",
    "            antonyms.append(l.antonyms()[0].name())\n",
    "\n",
    "print(set(synonyms))\n",
    "print(set(antonyms))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9090909090909091\n"
     ]
    }
   ],
   "source": [
    "w1= wordnet.synset(\"ship.n.01\")\n",
    "w2=wordnet.synset(\"boat.n.01\")\n",
    "\n",
    "#wup defines similarities between words\n",
    "print(w1.wup_similarity(w2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6956521739130435\n"
     ]
    }
   ],
   "source": [
    "w1= wordnet.synset(\"ship.n.01\")\n",
    "w2=wordnet.synset(\"car.n.01\")\n",
    "\n",
    "#wup defines similarities between words\n",
    "print(w1.wup_similarity(w2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.32\n"
     ]
    }
   ],
   "source": [
    "w1= wordnet.synset(\"ship.n.01\")\n",
    "w2=wordnet.synset(\"cat.n.01\")\n",
    "\n",
    "#wup defines similarities between words\n",
    "print(w1.wup_similarity(w2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
